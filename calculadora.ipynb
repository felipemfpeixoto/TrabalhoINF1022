{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de calculadora usando o PLY (Python Lex-Yacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ply.lex import lex\n",
    "from ply.yacc import yacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos uma calculadora para a seguinte gramatica:\n",
    "\n",
    "S' -> EXPRESSAO <br>\n",
    "EXPRESSAO -> numero OPERACAO numero <br>\n",
    "OPERACAO -> mais <br>\n",
    "OPERACAO -> menos <br>\n",
    "OPERACAO -> multiplicacao <br>\n",
    "OPERACAO -> divisao <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisador Lexico (Lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O analisador lexico define os tokens que serão usados na linguagem. A definição de Token para o nosso contexto são os terminais da gramática. Os tokens são definidos por duas coisas: A primeira é a uma lista ou tupla com o nome de variavel chamado `tokens`, a segunda é com nomes de variaveis ou por funções do tipo `t_NOME_TOKEN`, mas entre nome de variaveis e funções, independentemente da forma que escolhermos, devemos liga-los a uma expressão regular que define o padrão de cada token. Um detalhe importante é que **a ordem de definição dos tokens (t_NOME_TOKEN) é importante, ele sempre dará preferencia aos tokens definidos primeiro**, por isso é importante definir os tokens com menos expressividade primeiro. A baixo temos os tokens mais basicos que usaremos para o nosso exemplo de calculadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mais', 'menos', 'multiplicacao', 'divisao')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# nome dos tokens de operadores e constantes\n",
    "reservados = ('mais','menos','multiplicacao','divisao') \n",
    "\n",
    "# expressões regulares para tokens de operadores e constantes \n",
    "\n",
    "t_mais = r'\\+'\n",
    "t_menos = r'-'\n",
    "t_multiplicacao = r'\\*'\n",
    "t_divisao = r'/'\n",
    "\n",
    "reservados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui em baixo vamos definir numeros, como este é o nosso token de maior expressividade temos que defini-lo por ultimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mais', 'menos', 'multiplicacao', 'divisao', 'numero')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def t_numero(t): # aqui definimos o token numero, ele nesse caso converte o valor direto para um inteiro, mas poderia ser um float\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "t_ignore = ' \\t\\n' # ignora espaços e tabs\n",
    "\n",
    "def t_error(t): # nos dizer qual caractere ilegal e se tem erro\n",
    "    print(\"Caracter ilegal: \", t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "tokens = reservados + ('numero',)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos instanciar o nosso analisador lexico, no inicio vamos deixa-lo com o modo de debug ativado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lex: tokens   = ('mais', 'menos', 'multiplicacao', 'divisao', 'numero')\n",
      "lex: literals = ''\n",
      "lex: states   = {'INITIAL': 'inclusive'}\n",
      "lex: Adding rule t_numero -> '\\d+' (state 'INITIAL')\n",
      "lex: Adding rule t_mais -> '\\+' (state 'INITIAL')\n",
      "lex: Adding rule t_multiplicacao -> '\\*' (state 'INITIAL')\n",
      "lex: Adding rule t_menos -> '-' (state 'INITIAL')\n",
      "lex: Adding rule t_divisao -> '/' (state 'INITIAL')\n",
      "lex: ==== MASTER REGEXS FOLLOW ====\n",
      "lex: state 'INITIAL' : regex[0] = '(?P<t_numero>\\d+)|(?P<t_mais>\\+)|(?P<t_multiplicacao>\\*)|(?P<t_menos>-)|(?P<t_divisao>/)'\n"
     ]
    }
   ],
   "source": [
    "__file__ = 'calculadora.ipynb' # somente para funcionar no jupyter notebook\n",
    "\n",
    "lexer = lex(debug=True) # construção do lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o debug em true, podemos ver que ele mostra como ele internamente guarda os tokens (como `<t_nome_token>`), podemos ver tambem as expressões regulares assosiadas a ele.\n",
    "\n",
    "Antes de continuarmos, vale a pena relembrar que o PLY nos disponibiliza outras ferramentas como `literals`, aqui somente estará o minimo necessario para fazer o trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisador Sintatico (Yacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramatica de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gramatica de atributos é uma materia que está bem no final do curso, então se você esta lendo isso e não saber o que é, expliquei brevemente o que é e como funciona. Caso o contrario, pode pular para a proxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "S → S MAIS A <br>\n",
    "S → A <br>\n",
    "A → 1 <br>\n",
    "A → 0 <br>\n",
    "\n",
    "Se conectar funções a essa gramatica ficaria algo assim (ligação esta sendo representado por ⇛)\n",
    "\n",
    "S → S MAIS A ⇛ `lambda S,MAIS,A: S + A` <br>\n",
    "S → A ⇛ `lambda A: A` <br>\n",
    "A → 1 ⇛ `lambda: 1` <br>\n",
    "A → 0 ⇛ `lambda: 0` <br>\n",
    "\n",
    "Então se executarmos a gramatica de atributos para a entrada `1+1+1+0` teriamos o output igual a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a gramatica de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o PLY a gramatica é definida por funções com `p_REGRA_DA_GRAMATICA`, essas funções precisam de uma regra associada a ela, esta e criada a partir da `DEFINIÇÃO DE DOCUMENTAÇÃO DE FUNÇÃO DO PYTHON que se define entre 3 aspas no INICIO da função`, essas funções tem como parametro um array que a partir do index 1 são os tokens e o retorno de outras regras, o primeiro valor desse array (index 0) é o retorno da regra atual, para ficar mais claro, vamos ver um exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def p_INICIO(regras):\n",
    "    '''\n",
    "    INICIO : EXPRESSAO\n",
    "           | numero\n",
    "    '''\n",
    "    regras[0] = regras[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do codigo acima, ele é equivalente a dizermos:\n",
    "\n",
    "S → EXPRESSAO <br>\n",
    "S → num <br>\n",
    "\n",
    "Então agora vamos analisar a regra abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def p_EXPRESSAO(regras):\n",
    "    '''\n",
    "    EXPRESSAO : numero mais numero\n",
    "              | numero menos numero\n",
    "              | numero multiplicacao numero\n",
    "              | numero divisao numero\n",
    "              | numero\n",
    "    '''\n",
    "    if len(regras) == 2: # checa se é o caso de um número ou uma constante\n",
    "        regras[0] = regras[1]\n",
    "        \n",
    "    else: # checa se é o caso de uma operação\n",
    "        if regras[2] == '+':\n",
    "            regras[0] = regras[1] + regras[3]\n",
    "        elif regras[2] == '-':\n",
    "            regras[0] = regras[1] - regras[3]\n",
    "        elif regras[2] == '*':\n",
    "            regras[0] = regras[1] * regras[3]\n",
    "        elif regras[2] == '/':\n",
    "            regras[0] = regras[1] / regras[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que essa função ficou um pouco grande e com muitas regras, aconselho tomar um tempo lendo ela para entender bem como funciona a gramatica de atributos.\n",
    "\n",
    "Ao longo do trabalho vocês, desenvolvedores do analisador, poderam criar novas regras, inclusive para diminuir regras grandes, então eu abaixo nos quebraremos essa regra em regras menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_EXPRESSAO(regras):\n",
    "    '''\n",
    "    EXPRESSAO : numero OPERACAO numero\n",
    "    '''\n",
    "    \n",
    "    regras[0] = regras[2](regras[1], regras[3])\n",
    "    \n",
    "\n",
    "def p_OPERACAO(regras):\n",
    "    '''\n",
    "    OPERACAO : mais\n",
    "             | menos\n",
    "             | multiplicacao\n",
    "             | divisao\n",
    "    '''\n",
    "    if regras[1] == '+':\n",
    "        regras[0] = lambda x,y: x+y\n",
    "    elif regras[1] == '-':\n",
    "        regras[0] = lambda x,y: x-y\n",
    "    elif regras[1] == '*':\n",
    "        regras[0] = lambda x,y: x*y\n",
    "    elif regras[1] == '/':\n",
    "        regras[0] = lambda x,y: x/y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto agora temos mais regras, porem elas estão mais simples e mais faceis de entender, agora vamos tentar rodar nosso analisador lexico e sintatico juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'parsetab' has no attribute '_tabversion'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mp_error\u001b[39m(regras):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mErro de sintaxe\u001b[39m\u001b[33m\"\u001b[39m+ \u001b[38;5;28mstr\u001b[39m(regras))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m parser = \u001b[43myacc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# construção do parser\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PUC-Rio/Analisadores/TrabalhoFinal/venv/lib/python3.13/site-packages/ply/yacc.py:3293\u001b[39m, in \u001b[36myacc\u001b[39m\u001b[34m(method, debug, module, tabmodule, start, check_recursion, optimize, write_tables, debugfile, outputdir, debuglog, errorlog, picklefile)\u001b[39m\n\u001b[32m   3291\u001b[39m     read_signature = lr.read_pickle(picklefile)\n\u001b[32m   3292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3293\u001b[39m     read_signature = \u001b[43mlr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mor\u001b[39;00m (read_signature == signature):\n\u001b[32m   3295\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PUC-Rio/Analisadores/TrabalhoFinal/venv/lib/python3.13/site-packages/ply/yacc.py:1987\u001b[39m, in \u001b[36mLRTable.read_table\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m   1984\u001b[39m     exec(\u001b[33m'\u001b[39m\u001b[33mimport \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m % module)\n\u001b[32m   1985\u001b[39m     parsetab = sys.modules[module]\n\u001b[32m-> \u001b[39m\u001b[32m1987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparsetab\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tabversion\u001b[49m != __tabversion__:\n\u001b[32m   1988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VersionError(\u001b[33m'\u001b[39m\u001b[33myacc table file version is out of date\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1990\u001b[39m \u001b[38;5;28mself\u001b[39m.lr_action = parsetab._lr_action\n",
      "\u001b[31mAttributeError\u001b[39m: module 'parsetab' has no attribute '_tabversion'"
     ]
    }
   ],
   "source": [
    "def p_error(regras):\n",
    "    print(\"Erro de sintaxe\"+ str(regras))\n",
    "\n",
    "parser = yacc(debug=True) # construção do parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto temos um analisadore lexico e sintatico, agora vamos ver se ele consegue avaliar a expressão `3 * 5`.\n",
    "\n",
    "PS: Ele ta dando esses WARNINGS porque ele não precisa daquele INICIO que definimos no inicio, ele ja gera internamente um estado S, pode-se ver isso atraves do parser.out, que é um arquivo de debug do Yacc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('23 + 4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
